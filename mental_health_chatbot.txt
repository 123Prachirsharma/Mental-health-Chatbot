# -*- coding: utf-8 -*-
"""MENTAL HEALTH CHATBOT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15rLt8QvzPq2D-uh6cwGjT5Aix9QiYShj
"""

! pip install langchain_groq langchain_core langchain_community

from langchain_groq import ChatGroq

# Initialize the LLM with Groq
llm = ChatGroq(
    temperature=0,
    groq_api_key="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",  # Replace securely
    model_name="llama3-70b-8192"  # Updated to match Groq's supported models
)

# Invoke the LLM
result = llm.invoke("narendra modi")
print(result.content)

! pip install pypdf

!pip install chromadb

pip install -U langchain-community

! pip install sentence_transformers

!mkdir -p /content/data

from google.colab import files
uploaded = files.upload()  # Then move them manually or via code into /content/data

pip install langchain langchain-community langchain-groq chromadb sentence-transformers

!pip install gradio

import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq  # Make sure this is installed!
import gradio as gr


def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",
        model_name="llama3-70b-8192"
    )
    return llm


def create_vector_db():
    loader = DirectoryLoader("/content/data/", glob="*.pdf", loader_cls=PyPDFLoader)
    documents = loader.load()
    print(f"[INFO] Loaded {len(documents)} documents.")

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    print(f"[INFO] Split into {len(texts)} text chunks.")

    if not texts:
        raise ValueError("No text chunks were created. Please check if the PDFs contain extractable text.")

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    vector_db = Chroma.from_documents(texts, embeddings, persist_directory="/content/chroma_db")
    vector_db.persist()

    print("[SUCCESS] ChromaDB created and data saved.")
    return vector_db


def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt_template = """You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
{context}
user: {question}
chatbot:"""
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain


print("ðŸ§  Initializing Chatbot...")
llm = initialize_llm()

db_path = "/content/chroma_db"
if not os.path.exists(db_path):
    vector_db = create_vector_db()
else:
    print("[INFO] Loading existing vector database.")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)

qa_chain = setup_qa_chain(vector_db, llm)

def chatbot_response(user_input, history=[]):
    if not user_input.strip():
        return "Please provide a valid input", history
    response = qa_chain.run({"query": user_input})
    response_text = response['result']
    history.append((user_input, response_text))
    return "", history


with gr.Blocks(theme='Respait/shiki@1.2.1')as app:
  chatbot=gr.ChatInterface(fn=chatbot_response,title="Mental Health Buddy")

app.launch()

import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq

def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",
        model_name="llama3-70b-8192"
    )
    return llm

def create_vector_db():
    # Load PDF documents from the directory
    loader = DirectoryLoader("/content/data/", glob="*.pdf", loader_cls=PyPDFLoader)
    documents = loader.load()
    print(f"[INFO] Loaded {len(documents)} documents.")

    # Split documents into chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    print(f"[INFO] Split into {len(texts)} text chunks.")

    if not texts:
        raise ValueError("No text chunks were created. Please check if the PDFs contain extractable text.")

    # Initialize embeddings with the corrected model name
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # Create and persist the Chroma vector database
    vector_db = Chroma.from_documents(texts, embeddings, persist_directory="./chroma_db")
    vector_db.persist()

    print("ChromaDB created and data saved")
    return vector_db

def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt_template = """You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
{context}
user: {question}
chatbot:"""
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

def main():
    print("Initializing Chatbot.....")
    llm = initialize_llm()

    db_path = "/content/chroma_db"
    if not os.path.exists(db_path):
        vector_db = create_vector_db()
    else:
        print("[INFO] Loading existing vector database.")
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)

    qa_chain = setup_qa_chain(vector_db, llm)

    print("ðŸ¤– Chatbot is ready! Type your questions or 'exit' to quit.")
    while True:
        query = input("\nHuman: ")
        if query.lower() == "exit":
            print("Chatbot: Take care of yourself, goodbye")
            break
        response = qa_chain.invoke({"query": query})
        print(f"Chatbot: {response['result']}")

if __name__ == "__main__":
    main()

import os
import gradio as gr
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq
from langchain.vectorstores import Chroma  # You missed importing this in your code!

# Initialize the LLM
def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",
        model_name="llama3-70b-8192"
    )
    return llm

# Create or Load the Vector DB
def create_vector_db():
    loader = DirectoryLoader("/content/data/", glob="*.pdf", loader_cls=PyPDFLoader)
    documents = loader.load()
    print(f"[INFO] Loaded {len(documents)} documents.")

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    print(f"[INFO] Split into {len(texts)} text chunks.")

    if not texts:
        raise ValueError("No text chunks were created. Please check if the PDFs contain extractable text.")

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    vector_db = Chroma.from_documents(texts, embeddings, persist_directory="./chroma_db")
    vector_db.persist()

    print("[INFO] ChromaDB created and data saved")
    return vector_db

def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt_template = """You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
{context}
user: {question}
chatbot:"""
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

# Initialize everything
print("Initializing Chatbot.....")
llm = initialize_llm()

db_path = "./chroma_db"
if not os.path.exists(db_path):
    vector_db = create_vector_db()
else:
    print("[INFO] Loading existing vector database.")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)

qa_chain = setup_qa_chain(vector_db, llm)

# Define Gradio chat function
def chat(message, history):
    history = history or []
    response = qa_chain.invoke({"query": message})
    bot_message = response['result']

    history.append((message, bot_message))
    return history, history

# Create Gradio Interface
with gr.Blocks() as demo:
    gr.Markdown("<h1><center>ðŸ§  Compassionate Mental Health Chatbot</center></h1>")

    chatbot = gr.Chatbot()
    msg = gr.Textbox(placeholder="Type your message here...", label="Your Message")
    clear = gr.Button("Clear")

    state = gr.State([])

    msg.submit(chat, [msg, state], [chatbot, state])
    clear.click(lambda: ([], []), outputs=[chatbot, state])

demo.launch()